Korinek's paper identifies six key areas where large language models (LLMs) like ChatGPT can assist researchers: ideation and feedback, writing, background research, data analysis, coding, and mathematical derivations. Generative AI tools like these have crossed the threshold into practical utility for various cognitive tasks, allowing researchers to automate what Korinek refers to as "micro-tasks." These tasks—although minor individually—accumulate significantly throughout the research process, and automation can lead to notable productivity gains.

In my research on developing a two-dimensional Fama-French factor portfolio for Indian equities, I see several avenues where LLMs, particularly ChatGPT, can streamline my workflow. Primarily, I plan to use these tools to enhance my writing, debug Python scripts, and verify mathematical derivations and the core methodological framework. For example, during a recent data analysis exercise, I used ChatGPT to draft and troubleshoot scripts for my Jupyter Notebook. This rapid iteration, which otherwise would have been more time-consuming or required additional resources, significantly improved the efficiency of my work.

Korinek emphasizes that LLMs are most effective as assistants for automating small, repetitive tasks that human research assistants often do not handle due to the low scale and high frequency of these tasks. The productivity benefits I’ve experienced align with this observation, especially in overcoming technical challenges that arise during data analysis or coding. For instance, even minor issues like syntax errors or logic missteps, which often obstruct the progress of analysis, can be efficiently resolved using LLMs.

However, as Korinek points out, there are inherent risks in using generative AI, particularly concerning data privacy and model accuracy. I remain cautious about sharing proprietary information—such as datasets or specific portfolio methodologies—on the platform, mindful that the data could be retained and potentially used for future model training. This concern is particularly relevant as my research involves sensitive financial metrics that could lead to intellectual property exposure. Thus, I avoid entering any proprietary numerical data directly into LLMs, sticking instead to asking for general debugging support or broader questions.

Korinek also notes the dangers of both overestimating and underestimating LLMs' capabilities. While the models have grown increasingly adept at language processing and specific technical tasks, there is always a risk of "hallucinations"—situations where the AI generates content that sounds authoritative but is factually incorrect. This dual potential—of either over-relying on or dismissing AI tools—requires careful balance. I treat LLM-generated outputs as a starting point, always applying my critical judgment to verify their validity before using them in my research.

Overall, incorporating LLMs into my research has introduced meaningful improvements, but it has also required me to adopt a cautious, informed approach. Leveraging these tools effectively involves understanding their strengths in automating repetitive or low-level cognitive tasks while recognizing the limitations and risks associated with their use in sensitive contexts.
